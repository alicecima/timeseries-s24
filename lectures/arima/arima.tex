\documentclass{article}

\def\ParSkip{} 
\input{../../common/ryantibs}

\title{Lecture 6: Autoregressive Integrated Moving Average Models \\ \smallskip  
\large Introduction to Time Series, Fall 2023 \\ \smallskip
Ryan Tibshirani}
\date{}

\begin{document}
\maketitle
\RaggedRight
\vspace{-50pt}

Related reading: Chapters 3.1 and 3.3 Shumway and Stoffer (SS); Chapters
9.1--9.5 and 9.8--9.9 of Hyndman and Athanasopoulos (HA).   

\section{AR models}

\begin{itemize}
\item The \emph{autoregressive} (AR) model is one of the foundational legs of
  ARIMA models, which we'll cover bit by bit in this lecture. (Recall, you've
  already learned about AR models, which were introduced all the way back in our 
  first lecture)  

\item Precisely, an AR model of order $p \geq 0$, denoted AR($p$), is of the
  form 
  \begin{equation}
  \label{eq:ar-p}
  x_t = \sum_{j=1}^p \phi_j x_{t-j} + w_t
  \end{equation}
  where $w_t$, $t = 0, \pm 1, \pm 2, \pm 3, \dots$ is a white noise
  sequence. Note that we allow the time index to be negative here (we extend
  time back to $-\infty$), which will useful in what follows 

\item The coefficients $\phi_1,\dots,\phi_p$ in \eqref{eq:ar-p} are fixed
  (nonrandom), and we assume $\phi_p \not= 0$ (otherwise the order here would
  effectively be less than $p$). Note that in \eqref{eq:ar-p}, we have $\E(x_t)
  = 0$ for all $t$

\item If we wanted to allow for a nonzero but constant mean, then we could add
  an intercept to the model in \eqref{eq:ar-p}. We'll omit this for simplicity
  in this lecture  

\item A useful tool for expressing and working with AR models is the
  \emph{backshift operator}: this is an operator we denote by $B$ that takes a 
  given time series and shifts it back in time by one index,
  \[
  B x_t = x_{t-1}
  \]

\item We can extend this to powers, as in $B^2 x_t = B B x_t = x_{t-2}$, and so
  on, thus   
  \[
  B^k x_t = x_{t-k} 
  \]

\item Returning to \eqref{eq:ar-p}, note now that we can rewrite this as 
  \[
  x_t - \phi_1 x_{t-1} - \phi_2 - x_{t-2} - \cdots - \phi_p x_{t-p} = w_t  
  \]
  or in other words, using backshift notation 
  \begin{equation}
  \label{eq:ar-p-backshift}
  \Big(1 - \phi_1 B - \phi_2 B^2 - \cdots - \phi_p B^p \Big) x_t = w_t 
  \end{equation}

\item Hence \eqref{eq:ar-p-backshift} is just a compact way to represent the
  AR($p$) model \eqref{eq:ar-p} using the backshift operator $B$. Often, authors
  will write this model even more compactly as  
  \begin{equation}
  \label{eq:ar-p-operator}
  \phi(B) x_t = w_t 
  \end{equation}
  where $\phi(B) = 1 - \phi_1 B - \phi_2 B^2 - \cdots - \phi_p B^p$ is called
  the \emph{autoregressive operator} of order $p$, associated with the
  coefficients $\phi_1,\dots,\phi_p$

\item Figure \ref{fig:ar} shows two simple examples of AR processes
\end{itemize}

\begin{figure}[htb]
\centering
\includegraphics[width=0.85\textwidth]{fig/ar-1.pdf}
\caption{Two examples of AR(1) processes, with $\phi = \pm 0.9$.}
\label{fig:ar}
\end{figure}

\subsection{AR(1): auto-covariance and stationarity}

\begin{itemize}
\item A key question for us will be: \emph{under what conditions does the AR
    model  in \eqref{eq:ar-p}, or equivalently \eqref{eq:ar-p-operator}, define
    a stationary process?} 

\item The answer will turn out to be fairly sophisticated, but we can glean some
  intuition by starting with the AR(1) case: 
  \begin{equation}
  \label{eq:ar-1}
  x_t = \phi x_{t-1} + w_t 
  \end{equation}

\item Unraveling the iterations, we get
  \begin{align*}
  x_t &= \phi^2 x_{t-2} + \phi w_{t-1} + w_t \\
  &= \phi^3 x_{t-3} + \phi^2 w_{t-2} + \phi w_{t-1} + w_t \\  
  &\vdots \\ 
  &= \phi^k x_{t-k} + \sum_{j=0}^k \phi^j w_{t-j} 
  \end{align*}

\item If $|\phi| < 1$, then we can send $k \to \infty$ in the last display to 
  get 
  \begin{equation}
  \label{eq:ar-1-stationary}
  x_t = \sum_{j=0}^\infty \phi^j w_{t-j} 
  \end{equation}
  This is called the \emph{stationary representation} of the AR(1) process
  \eqref{eq:ar-1} 

\item Why is it called this? We can compute the auto-covariance function,
  writing $\sigma^2 = \Var(w_t)$ for the noise variance, as
  \begin{align}
  \nonumber
  \Cov(x_t, x_{t+h}) &= \Cov\bigg( \sum_{j=0}^\infty \phi^j w_{t-j}, 
  \sum_{\ell=0}^\infty \phi^\ell w_{t+h-\ell} \bigg) \\
  \nonumber
  &= \sum_{j,\ell=0}^\infty \phi^j \phi^\ell \Cov( w_{t-j}, w_{t+h-\ell} ) \\ 
  \nonumber
  &= \sum_{j=0}^\infty \phi^j \phi^{j+h} \sigma^2 \\
  \nonumber
  &= \sigma^2 \phi^h \sum_{j=0}^\infty \phi^{2j} \\
  \label{eq:ar-1-auto-cov}
  &= \sigma^2 \frac{\phi^h}{1 - \phi^2}    
  \end{align}
  where we used the fact that \smash{$\sum_{j=0}^\infty b^j = 1/(1-b)$} for $|b|
  < 1$. Since the auto-covariance in the last line only depends on $h$, we can
  see that the AR(1) process is indeed stationary

\item To reiterate, the representation \eqref{eq:ar-1-stationary}, and the
  auto-covariance calculation just given, would have not been possible unless
  $|\phi| < 1$. This condition is required in order for the AR(1) process to
  have a stationary representation. We will see later that we can generalize
  this to a condition that applies to an AR($p$), yielding an analogous
  conclusion. The conclusion we will be looking for is explained next  
\end{itemize}

\subsection{Causality (no, not the usual kind)}

\begin{itemize}
\item Now we will introduce a concept called \emph{causality}, which generalizes
  what we just saw falls out of an AR(1) when $|\phi| < 1$. This is a slightly
  unfortunate bit of nomenclature that nonetheless seems to be common in the
  time series literature. It has really nothing to do with causality used in the
  broader sense in statistics. We will ... somewhat begrudgingly ... stick with
  the standard nomenclature in time series here  

\item We say that a series $x_t$, $t = 0, \pm 1, \pm 2, \pm 3, \dots$ is
  \emph{causal} provided that it can be written in the form
  \begin{equation}
  \label{eq:causal}
  x_t = \sum_{j=0}^\infty \psi_j w_{t-j}
  \end{equation}
  for a white noise sequence $w_t$, $t = 0, \pm 1, \pm 2, \pm 3, \dots$, and
  coefficients such that \smash{$\sum_{j=0}^\infty |\psi_j| < \infty$}  

\item You should think of this as a generalization of
  \eqref{eq:ar-1-stationary}, where we allow for arbitrary coefficients
  $\psi_0,\psi_1,\psi_2,\dots$, subject to an absolute summability condition  

\item It is straightforward to check that causality actually implies
  stationarity: we can just compute the auto-covariance function in
  \eqref{eq:causal}, similar to the above calculation:
    \begin{align*}
  \Cov(x_t, x_{t+h}) &= \Cov\bigg( \sum_{j=0}^\infty \psi_j w_{t-j}, 
  \sum_{\ell=0}^\infty \psi_\ell w_{t+h-\ell} \bigg) \\
  &= \sum_{j,\ell=0}^\infty \psi_j \psi_\ell \Cov( w_{t-j}, w_{t+h-\ell} ) \\
  &= \sigma^2 \sum_{j=0}^\infty \psi_j \psi_{j+h}
  \end{align*}
  The summability condition ensures that these calculations are well-defined and
  that the last display is finite. Since this only depends on $h$, we can see
  that the process is indeed stationary

\item Thus, to emphasize, causality actually tells us \emph{more} than
  stationary: it is stationary ``plus'' a representation a linear filter of past
  white noise variates, with summable coefficients

\item Note that when $\psi_j = \phi^j$, the summability condition
  \smash{$\sum_{j=0}^\infty |\psi_j| < \infty$} is true if and only if $|\phi| <
  1$. Hence what we actually proved above for AR(1) was that it is causal if and
  only if $|\phi| < 1$. And it is this condition---for causality---that we will
  actually generalize for AR($p$) models, and beyond 
\end{itemize}

\section{MA models}

\begin{itemize}
\item A \emph{moving average} (MA) model is ``dual'', in a colloquial sense, to
  the AR model. Instead of having $x_t$ evolve according to a linear combination
  of the recent past, the \emph{errors} in the model evolve according to a
  linear combination of white noise

\item Precisely, an MA model of order $q \geq 0$, denoted MA($q$), is of the
  form 
  \begin{equation}
  \label{eq:ma-q}
  x_t = w_t + \sum_{j=1}^q \theta_j w_{t-j} 
  \end{equation}
  where $w_t$, $t = 0, \pm 1, \pm 2, \pm 3, \dots$ is a white noise
  sequence

\item The coefficients $\theta_1,\dots,\theta_q$ in \eqref{eq:ma-q} are fixed
  (nonrandom), and we assume $\theta_q \not= 0$ (otherwise the order here would 
  effectively be less than $q$). Note that in \eqref{eq:ma-q}, we have $\E(x_t)
  = 0$ for all $t$ 

\item Again, we can rewrite \eqref{eq:ma-q}, using backshift notation, as 
  \begin{equation}
  \label{eq:ma-q-backshift}
  x_t = \Big(1 + \theta_1 B + \theta_2 B^2 + \cdots + \theta_q B^q \Big) w_t   
  \end{equation}

\item Often, authors will write \eqref{eq:ma-q-backshift} even more compactly as   
  \begin{equation}
  \label{eq:ma-q-operator}
  x_t = \theta(B) w_t 
  \end{equation}
  where $\theta(B) = 1 + \theta_1 B + \theta_2 B^2 + \cdots + \theta_q B^q$ is
  called the \emph{moving average operator} of order $q$, associated with the
  coefficients $\theta_1,\dots,\theta_q$

\item Figure \ref{fig:ma} shows two simple examples of MA processes

\begin{figure}[htb]
\centering
\includegraphics[width=0.85\textwidth]{fig/ma-1.pdf}
\caption{Two examples of MA(1) processes, with $\theta = \pm 0.9$.}
\label{fig:ma}
\end{figure}
\end{itemize}

\subsection{Stationarity}

\begin{itemize}
\item Unlike AR processes, an MA process \eqref{eq:ma-q} is stationary \emph{for
    any values of the parameters $\theta_1,\dots,\theta_q$}

\item To check this, we compute the auto-covariance function using a similar
  calculation to those we've done before, writing $\theta_0 = 1$ for
  convenience:  
  \begin{align}
  \nonumber
  \Cov(x_t, x_{t+h}) &= \Cov\bigg( \sum_{j=0}^q \theta_j w_{t-j}, 
  \sum_{\ell=0}^q \theta_\ell w_{t+h-\ell} \bigg) \\
  \nonumber
  &= \sum_{j,\ell=0}^q \theta_j \theta_\ell \Cov( w_{t-j}, w_{t+h-\ell} ) \\
  \label{eq:ma-q-auto-cov}
  &= \sigma^2 \sum_{j=0}^q \theta_j \theta_{j+h}
  \end{align}
  Since this only depends on $h$, we can see that the process is indeed
  stationary 

\item The similarity in these calculations brings us to pause to emphasize the
  following connection: \emph{an AR(1) model with $|\phi| < 1$ is also a
    particular infinite-order MA model}, as we saw in the stationary
  representation \eqref{eq:ar-1-stationary}. We will see later that there are
  more general connections to be made
\end{itemize}

\subsection{MA(1): issues with non-uniqueness}

\begin{itemize}
\item Consider the MA(1) model:
  \begin{equation}
  \label{eq:ma-1}
  x_t = w_t + \theta w_{t-1}
  \end{equation}

\item According to \eqref{eq:ma-q-auto-cov}, we can compute its auto-covariance 
  simply (recalling $\theta_0 = 1$) as
  \begin{equation}
  \label{eq:ma-1-auto-cov}  
  \gamma(h) = \begin{cases}
  (1+\theta^2) \sigma^2 & h = 0 \\
  \theta \sigma^2 & |h| = 1 \\
  0 & |h| > 1 \\
  \end{cases}
  \end{equation}

\item The corresponding auto-correlation function is thus
  \[
  \rho(h) = \begin{cases}
  1 & h = 0 \\
  \frac{\theta}{1+\theta^2} & |h| = 1 \\ 
  0 & |h| > 1 \\
  \end{cases}
  \]

\item If we look carefully, then we can see a problem lurking here: the
  auto-correlation function is unchanged if we replace $\theta$ by $1/\theta$

\item And in fact, the auto-covariance function \eqref{eq:ma-1-auto-cov} is
  unchanged if we replace $\theta$ and $\sigma^2$ with $1/\theta$ and $\sigma^2
  \theta^2$; e.g., try $\theta = 5$ and $\sigma^2 = 1$, and $\theta = 1/5$ and
  $\sigma^2 = 25$, you'll find that the auto-covariance function is the same in
  both cases 

\item This is not good because it means we cannot detect the difference in an
  MA(1) model with parameter $\theta$ and normal noise with variance $\sigma^2$ 
  from another MA(1) model with parameter $1/\theta$ and normal noise with
  variance $\sigma^2 \theta^2$

\item In other words, there is some \emph{non-uniqueness} of \emph{redundancy}
  in the parametrization---different choices of parameters will actually lead to
  the same behavior in the model at the end

\item In the MA(1) case, the convention is to simply choose the parametrization
  with $|\theta| <  1$. Note that we can write
  \[
  w_t = -\theta w_{t-1} + x_t 
  \]
  which is like an AR(1) process with the roles of $x_t$ and $w_t$
  reversed. Thus by the same arguments that led to \eqref{eq:ar-1-stationary},
  when $|\theta| < 1$, we now have 
  \begin{equation}
  \label{eq:ma-1-invertible}
  w_t = \sum_{j=0}^\infty (-\theta)^j x_{t-j} 
  \end{equation}
  This is called the \emph{invertible representation} of the MA(1) process
  \eqref{eq:ma-1} 

\item We will see soon that we can generalize this to a condition that applies
  to a general MA($q$), yielding an analogous conclusion. The conclusion we will
  be looking for is explained next 
\end{itemize}

\subsection{Invertibility}

\begin{itemize}
\item Before we turn to ARMA models, we define one last concept called
  \emph{invertibility}, which generalizes what we just saw for MA(1) when
  $|\theta| < 1$ 

\item We say that a series $x_t$, $t = 0, \pm 1, \pm 2, \pm 3, \dots$ is
  \emph{invertible} provided that it can be written in the form
  \begin{equation}
  \label{eq:invertible}
  w_t = \sum_{j=0}^\infty \pi_j x_{t-j}
  \end{equation}
  for a white noise sequence $w_t$, $t = 0, \pm 1, \pm 2, \pm 3, \dots$,
  and coefficients such that \smash{$\sum_{j=0}^\infty |\pi_j| < \infty$}, where 
  we set $\pi_0 = 1$

\item You should think of this as a generalization of
  \eqref{eq:ma-1-invertible}, where we allow for arbitrary coefficients
  $\pi_1,\pi_2,\dots$, subject to an absolute summability condition   

\item And of course, note how invertibility \eqref{eq:invertible} is kind of an
  opposite condition to causality \eqref{eq:causal}  
\end{itemize}

\section{ARMA models}

\begin{itemize}
\item AR and MA models have complementary characteristics. The auto-covariance
  of an AR model generally decays away from $h=0$, whereas that for an MA
  process has finite support---in other words, at a certain lag, variates along
  an MA sequence are completely uncorrelated. You can compare
  \eqref{eq:ar-1-auto-cov} and \eqref{eq:ma-1-auto-cov} for the AR(1) and MA(1)
  models

\item (The spectral perspective, by the way, provides another nice way of
  viewing these complementary characteristics. In the spectral domain, the story
  is somewhat flipped: the spectral density of an MA process generally decays
  away from $\omega=0$, whereas that for an AR process can be much more locally
  concentrated around particular frequencies; recall our examples from the last
  lecture) 

\item Sure, there is some duplicity in representation here, as we will see---we
  can write some AR models as infinite-order MA models, and some MA models as  
  infinite-order AR models. But that's OK! We can generally take the most
  salient features that each model represents, and combine them to get a simple
  model formulation that exhibits both sets of features, simultaneously. This is
  exactly what an ARMA model does      

\item Precisely, an ARMA model of orders $p,q \geq 0$, denoted ARMA($p,q$), is
  of the form 
  \begin{equation}
    \label{eq:arma-pq}
  x_t = \sum_{j=1}^p \phi_j x_{t-j} + \sum_{j=0}^q \theta_j w_{t-j}  
  \end{equation}
  where $w_t$, $t = 0, \pm 1, \pm 2, \pm 3, \dots$ is a white noise sequence 

\item The coefficients $\phi_1,\dots,\phi_p,\theta_0,\dots,\theta_q$ in
  \eqref{eq:arma-pq} are fixed (nonrandom), and we assume $\phi_p,\theta_q \not=
  0$, and we set $\theta_0 = 1$. Note that in \eqref{eq:arma-pq}, we have
  $\E(x_t) = 0$ for all $t$ 

\item As before, we can represent an ARMA model more compactly using backshift 
  notation, rewriting \eqref{eq:arma-pq} as
  \begin{equation}
  \label{eq:arma-pq-backshift}
  \Big(1 - \phi_1 B - \phi_2 B^2 - \cdots - \phi_p B^p \Big) x_t = 
  \Big(1 + \theta_1 B + \theta_2 B^2 + \cdots + \theta_q B^q \Big) w_t   
  \end{equation}

\item Often, authors will write \eqref{eq:arma-pq-backshift} even more compactly
  as    
  \begin{equation}
  \label{eq:arma-pq-operator}
  \phi(B) x_t = \theta(B) w_t  
  \end{equation}
  where  $\phi(B) = 1 - \phi_1 B - \phi_2 B^2 - \cdots - \phi_p B^p$ and
  $\theta(B) = 1 + \theta_1 B + \theta_2 B^2 + \cdots + \theta_q B^q$ are the AR
  and MA operators, respectively, as before 
\end{itemize}

\subsection{Parameter redundancy}

\begin{itemize}
\item For ARMA models, there is an issue of parameter redundancy, just like
  there are for MA models. If $\eta(B)$ is any (invertible) operator, then we
  can transform \eqref{eq:arma-pq-operator} by applying $\eta(B)$ on both sides,  
  \[
  \eta(B) \phi(B) x_t = \eta(B) \theta(B) w_t  
  \]
  which may look like a different model, but the dynamics are entirely the
  same  

\item As an example, consider white noise $x_t = w_t$, and multiply both sides
  by $\eta(B) = 1 - B/2$. This gives:
  \[
  x_t = \frac{1}{2} x_{t-1} + w_t - \frac{1}{2} w_{t-1}
  \]
  which looks like an ARMA(1,1) model, but it is nothing else that white noise!
 
\item How do we resolve this issue? Doing so requires introducing another
  concept. The \emph{AR and MA polynomials} associated with the ARMA process
  \eqref{eq:arma-pq} are 
  \begin{align}
  \label{eq:ar-p-polynomial}
  \phi(z) &= 1 - \phi_1 z - \phi_2 z^2 - \cdots - \phi_p z^p \\
  \label{eq:ma-q-polynomial}
  \theta(z) &= 1 + \theta_1 z + \theta_2 z^2 + \cdots + \theta_q z^q
  \end{align}
  respectively. To be clear, these are polynomials over \emph{complex numbers}
  $z \in \C$. (Note that these are just what we get by taking the AR and MA
  operators, and replacing the backshift operator $B$ by a complex argument $z$) 

\item As it turns out, several important properties of ARMA models can be 
  derived by placing conditions on the AR and MA polynomials. Here we'll see the
  first one, to deal with parameter redundancy: \emph{when we speak of an ARMA
    model, we implicitly assume that the AR and MA polynomials $\phi(z)$ and
    $\theta(z)$, in \eqref{eq:ar-p-polynomial} and \eqref{eq:ma-q-polynomial},
    have no common factors}. This rules out a case like   
  \[
  x_t = \frac{1}{2} x_{t-1} + w_t - \frac{1}{2} w_{t-1}
  \]
  because the polynomials each have $1-z/2$ as a common factor. Hence we do not  
  even refer to the above as an ARMA(1,1) model
\end{itemize}

\subsection{Causality and invertibility}

\begin{itemize}
\item Now we learn about two more conditions on the AR and MA polynomials that
  imply important general properties of the underlying ARMA process, and
  generalize calculations we saw earlier for AR(1) and MA(1) models

\item Before we describe these, we recall the following terminology: for a
  polynomial \smash{$P(z) = \sum_{j=0}^k a_j z^j$}, we say that $z$ is a
  \emph{root} of $P$ provided $P(z) = 0$

\item And we say that a point $z \in \C$ lies \emph{outside of the unit circle} 
  (in the complex plane $\C$) provided that $|z| > 1$, where $|z|$ is the
  complex modulus of $z$ (recall $|z|^2 = \mathrm{Re}\{z\}^2 +
  \mathrm{Im}\{z\}^2$)     

\item The first property: \emph{the ARMA process \eqref{eq:arma-pq} has a causal
    representation \eqref{eq:causal} if and only if all roots of the AR
    polynomial \eqref{eq:ar-p-polynomial} lie outside of the unit circle}. The
  coefficients $\psi_0,\psi_1,\psi_2,\dots$ in the causal representation can be
  determined by solving  
  \[
  \psi(z) = \phi(z)^{-1} \theta(z) \iff 
  \sum_{j=0}^\infty \psi_j z^j = \frac{1 + \theta_1 z + \theta_2 z^2 + \cdots +
    \theta_q z^q}{1 - \phi_1 z - \phi_2 z^2 - \cdots - \phi_p z^p}, \quad
  \text{for $|z| < 1$}
  \]

\item The second property: \emph{the ARMA process \eqref{eq:arma-pq} is
    invertible \eqref{eq:invertible} if and only if all roots of the MA
    polynomial \eqref{eq:ma-q-polynomial} lie outside of the unit circle}. The  
  coefficients $\pi_1,\pi_2,\dots$ in the invertible representation can be 
  determined by solving   
  \[
  \pi(z) = \theta(z)^{-1} \phi(z) \iff 
  \sum_{j=0}^\infty \pi_j z^j = \frac{1 - \phi_1 z - \phi_2 z^2 - \cdots -
    \phi_p z^p}{1 + \theta_1 z + \theta_2 z^2 + \cdots + \theta_q z^q}, \quad 
  \text{for $|z| < 1$}
  \]

\item (As an aside, you can see that parameter redundancy issues would not
  affect the causal and invertible representations, as they shouldn't, because
  any common factors in $\phi(z)$ and $\theta(z)$ would cancel in their ratios 
  which determine the coefficients in the causal and invertible expansions)  

\item Interestingly, these results can also be interpreted as follows:
  \begin{itemize}
  \item An AR($q$) process, such that the roots of $\phi$ lie outside the unit
    circle, can also be written as an MA($\infty$) process (this is what
    causality \eqref{eq:causal} means)

  \item An MA($q$) process, such that the roots of $\theta$ lie outside the 
  unit circle, can also be written as an AR($\infty$) process (this is what 
  invertibility \eqref{eq:invertible} means, as it says
  \smash{$x_t = -\sum_{j=1}^\infty \pi_j x_{t-j} + w_t$})
  \end{itemize}

\item We won't cover the proofs of these properties or go into further details
  about them. But you will see several hints of their significance (what the
  causal and invertible representations allow us to do) in what follows. At 
  a high-level, they are worth knowing because they are considered foundational 
  results for ARMA modeling (just like it is worth knowing foundations for 
  regression modeling, and so on). You can refer to Appendix B.2 of SS for
  proofs, or read more in Chapter 3 of SS

\item Also, recall that causality implies stationarity, so what we have just
  learned is the general result that renders an ARMA($p,q$) process stationary 

\item Finally, as a summary, here are three equivalent ways to represent a
  causal, invertible ARMA($p,q$) model:
  \begin{align*}
  &\phi(B) x_t = \theta(B) w_t \\
  &x_t = \psi(B) w_t, \quad \text{for $\psi(B) = \phi(B)^{-1} \theta(B)$} \\ 
  &\pi(B) x_t = w_t, \quad \text{for $\pi(B) = \theta(B)^{-1} \phi(B)$} 
  \end{align*}
\end{itemize}

\subsection{Auto-covariance}

\begin{itemize}
\item The auto-covariance for an MA($q$) model was already given in
  \eqref{eq:ma-q-auto-cov}. We can see that it has a finite bandwidth $2q+1$:
  this is a signature structure of the MA($q$) model

\item The auto-covariance for a causal AR(1) model was given in
  \eqref{eq:ar-1-auto-cov}. We can see that it decays away from $h=0$: this is 
  a signature structure of AR models. But what does the precise auto-covariance
  look like for a general causal AR($p$) model? How about a general causal
  ARMA($p,q$) model?  

\item The answer is much more complicated, but still possible to characterize
  precisely. We'll do it first for an AR($p$) model, and then look at an
  ARMA($p,q$) model, which will have an analogous behavior 

\item For an AR($p$) model \eqref{eq:ar-p}, assumed causal (i.e., all roots of
  $\phi(z)$ are outside of the unit circle), we can focus on the auto-covariance  
  at lags $ h \geq 0$ (without a loss of generality, because $\gamma$ is
  symmetric around zero), 
  \[
  \Cov(x_t, x_{t+h}) = \sum_{j=1}^p \phi_j \Cov(x_t, x_{t+h-j}) + \Cov(x_t,
  w_{t+h})  
  \]

\item The last term on the right-hand side zero for $h>0$; recall, $x_t$ is a  
  causal process, and only depends on white noise in the past. On the other
  hand, when $h=0$, writing \smash{$x_t = \sum_{j=0}^\infty \psi_j
    w_{t-j}$}, 
  \[
  \Cov(x_t, w_t) = \sum_{j=0}^\infty \psi_j \Cov(w_{t-j}, w_t) =  \sigma^2
  \psi_0   
  \]

\item Combining the last two displays, we learn that the auto-covariance
  function satisfies 
  \begin{align*}
  &\gamma(h) - \sum_{j=1}^p \phi_j \gamma(h-j) = 0, \quad h > 0 \\ 
  &\gamma(0) - \sum_{j=1}^p \phi_j \gamma(-j) = \sigma^2 \psi_0, 
  \end{align*}

\item This is called a \emph{difference equation} of order $p$ for the
  auto-covariance function $\gamma$. Some simple difference equations can be
  solved explicitly without complicated mathematics, but to solve a difference  
  equation in general requires knowing something (again) about the roots of a
  certain complex polynomial associated with the difference equation, when it is
  represented in operator notation. You can read Chapter 3.2 of SS for an
  introduction to the theory of difference equations

\item For an AR($p$) model, it turns out we can solve the difference equation in
  the 
  last display and get
  \begin{equation}
  \label{eq:ar-p-auto-cov}
  \gamma(h) = P_1(h) z_1^{-h} + \cdots + P_r(h) z_r^{-h}
  \end{equation}
  where each $P_j$ is a polynomial, and each $z_j$ is a root of the AR
  polynomial $\phi$. Because each $|z_j| > 1$ (all roots lie outside the unit
  circle), we see that the auto-covariance function will decay to zero as $h \to
  \infty$. In the case that some roots are complex, then what will actually 
  happen is that the auto-covariance will dampen to zero but in doing so
  oscillate in a sinusoidal fashion

\item Now what about an ARMA($p,q$) model \eqref{eq:arma-pq}, assumed
  causal? Similarly, we can focus on the auto-covariance a lags $h \geq 0$, 
  \[
  \Cov(x_t, x_{t+h}) = \sum_{j=1}^p \phi_j \Cov(x_t, x_{t+h-j}) + \sum_{j=0}^q 
  \theta_j \Cov(x_t, w_{t+h-j})  
  \]

\item The last term on the right-hand side zero for $h>q$, whereas when $h \leq
  q$, writing \smash{$x_t = \sum_{\ell=0}^\infty \psi_\ell  w_{t-\ell}$}, we see
  that for $j \geq h$, 
  \[
  \Cov(x_t, w_{t+h-j}) = \sum_{\ell=0}^\infty \psi_\ell \Cov(w_{t-\ell},
  w_{t+h-j}) = \sigma^2 \psi_{j-h}   
  \]

\item Combining the last two displays, we learn that the auto-covariance
  function satisfies 
  \begin{alignat*}{2}
  &\gamma(h) - \sum_{j=1}^p \phi_j \gamma(h-j) = 0, \quad &&h > q \\ 
  &\gamma(h) - \sum_{j=1}^p \phi_j \gamma(h-j) = \sigma^2 \sum_{j=h}^q
    \psi_{j-h}, \quad &&h \leq q
  \end{alignat*}

\item This is again a difference equation of order $p$ that determines $\gamma$,
  but the boundary condition (what happens when $h \leq q$) is more
  complicated. Nonetheless, the solution is still the form
  \eqref{eq:ar-p-auto-cov}, and the qualitative behavior is still the same as in
  the AR($p$) case   

\item Figure \ref{fig:auto-cor}, top row, shows sample auto-correlation
  functions for simple MA and AR models 

\begin{figure}[htb]
\centering
\includegraphics[width=0.85\textwidth]{fig/auto-cor-1.pdf}
\caption{Top row: sample auto-correlation functions for data from AR(2) and
  MA(3) models. Bottom row: sample partial auto-correlation functions for these
  same data.} 
\label{fig:auto-cor}
\end{figure}
\end{itemize}

\subsection{Partial auto-covariance}

\begin{itemize}
\item The auto-covariance function for an MA model provides a considerable
  amount of information that will help identify its structure: since it is zero
  for lags $h>q$, if we were to compute a sample version based on data, then by
  seeing where the sample auto-correlation ``cuts off'', we could roughly
  identify the order $q$ of the underlying MA process (see top right of Figure 
  \ref{fig:auto-cor} again)   

\item For an AR (or ARMA) model, this is not the case. As we saw from
  \eqref{eq:ar-p-auto-cov}, the auto-covariance decays to zero, but this tells
  us little about the AR order of dependence $p$ (see also top left of Figure 
  \ref{fig:auto-cor}). Thus it is worth pursuing a type of \emph{modified}
  correlation function for the AR model that behaves like the auto-correlation
  does for the MA model 

\item Such a modification will be given to us by the \emph{partial
    auto-correlation function}. In general, the partial correlation between
  random variables $X,Y$ given $Z$ is denoted \smash{$\rho_{XY|Z}$} and defined
  as    
  \begin{align*}
  &\rho_{XY|Y} = \Cor(X - \hat{X}, Y - \hat{Y}), \quad \text{where} \\
  &\text{$\hat{X}$ is the linear regression of $X$ on $Z$}, \quad \text{and} \\
  &\text{$\hat{Y}$ is the linear regression of $Y$ on $Z$}
  \end{align*}
  Here, and in what follows, by ``linear regression'' we mean regression in the
  population sense, so that precisely \smash{$\hat{X} = Z^\T \Cov(Z)^{-1}
    \E(ZX)$} and \smash{$\hat{Y} = Z^\T \Cov(Z)^{-1} \E(ZY)$}   

\item Said differently, the partial correlation of two random variables given
  $Z$ is the correlation \emph{after we remove (``partial out'') the linear
    dependence of each random variable on $Z$}     

\item We note that when $X,Y,Z$ are jointly normal, then this definition
  coincides with conditional correlation: \smash{$\rho_{XY|Z} = \Cor(X,Y|Z)$},
  but not in general 

\item We are now ready to define the partial auto-correlation function for a
  stationary time series $x_t$, $t = 0, \pm 1, \pm 2, \pm 3, \dots$, denoted
  $\phi_x(h)$ at a lag $h$. Without a loss of generality we will only define it
  for $h \geq 0$, since it will be symmetric around zero (due to stationarity). 
  First, at lag $h=0$ or $h=1$, we simply define: 
  \begin{align*}
  \phi_x(0) &= 1 \\
  \phi_x(1) &= \Cor(x_t, x_{t+1}) 
  \end{align*}
  Next, at all lags $h \geq 2$, we define: 
  \begin{align*}
  &\phi_x(h) = \Cor(x_t - \hat{x}_t, x_{t+h} - \hat{x}_{t+h}), \quad
    \text{where} \\ 
  &\text{$\hat{x}_t$ is the linear regression of $x_t$ on
    $x_{t+1},\dots,x_{t+h-1}$}, \quad \text{and} \\ 
  &\text{$\hat{x}_{t+h}$ is the linear regression of $x_{t+h}$ on 
    $x_{t+1},\dots,x_{t+h-1}$}  
  \end{align*}

\item To best see the effect of this definition we can go straight back to the
  causal AR($p$) model. When $h>p$, it can be shown that the population linear 
  regression \smash{$\hat{x}_{t+h}$}, of $x_{t+h}$ on $x_{t+1},\dots,x_{t+h-1}$, 
  is 
  \[
  \hat{x}_{t+h} = \sum_{j=0}^p \phi_j x_{t-j}
  \]
  Thus \smash{$x_{t+h} - \hat{x}_{t+h} = x_{t+h} -  \sum_{j=0}^p \phi_j x_{t-j}
    = w_{t+h}$}, and the partial auto-correlation is
  \[
  \phi_x(h) = \Cor(x_t - \hat{x}_t, w_{t+h}) = 0
  \]
  because causality implies that $x_t$ can only depend on white noise through
  time $t$, and \smash{$\hat{x}_t$} can only depend on white noise through time
  $t+h-1$ 

\item That is, the \emph{partial auto-correlation function for an AR($p$) model
  is exactly zero at all lags $h > p$}

\item Figure \ref{fig:auto-cor}, bottom row, shows sample partial
  auto-correlation functions for AR and MA models

\item The table below summarizes the behavior of the auto-correlation function
  (ACF) and partial auto-correlation function (PACF) for causal AR($p$) and
  invertible MA($q$) models. By ``tails off'' we mean decays to zero as $h \to
  \infty$ without dropping to zero exactly; by ``cuts off'' we mean drops to
  zero at a finite lag $h$
  \begin{center} \smallskip
  \begin{tabular}{c|c|c|c|}
  & AR($p$) & MA($q$) & ARMA($p,q$) \\
  \hline
  ACF & tails off & drops off at lag $q$ & tails off \\
  \hline
  PACF & drops off at lag $p$ & tails off & tails off \\
  \hline
  \end{tabular} \smallskip
  \end{center}
  The fact that the partial auto-correlation function for an invertible MA($q$)
  model ``tails off'' was not derived in these notes, but you can read more in
  Section 3.3 of SS if you are curious. Same with the behavior of a causal,
  invertible ARMA($p,q$) 
\end{itemize}

\subsection{Estimation and selection}

\subsection{Regression with correlated errors}

\section{ARIMA models}

Stationarity and differencing
ARIMA(p,d,q)
Backshift notation

\subsection{Seasonality extensions}

\subsection{IMA (EWMA) models}

connection to exponential something

\section{Forecasting}

ARMAX models

Long-range forecasts converge to zero (or to the mean, in a process with 
nonzero mean)

\end{document}
