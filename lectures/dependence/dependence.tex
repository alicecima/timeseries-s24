\documentclass{article}

\def\ParSkip{} 
\input{../../common/ryantibs}

\title{Lecture 2: Measures of Dependence and Stationarity \\ \smallskip  
\large Introduction to Time Series, Fall 2023 \\ \smallskip
Ryan Tibshirani}
\date{}

\begin{document}
\maketitle
\RaggedRight
\vspace{-50pt}

Related reading: Chapters 1.3--? of Shumway and Stoffer (SS).

\section{Mean and variance}

\begin{itemize}
\item Given a sequence $x_t$, $t = 1,2,3,\dots$, we define its \emph{mean 
    function} (this is viewed as a function of time) by 
  \[
  \mu_{x,t} = \E(x_t)
  \]
  When it is unambiguous from the context which underlying sequence it refers
  to, we drop the first subscript and simply denote this by $\mu_t$

\item Moreover, we define its \emph{variance function} by
  \[
  \sigma^2_{x,t} = \Var(x_t) = \E[(x_t - \mu_t)^2]
  \]
  Again, when the underlying sequence should be clear from the context, we
  simplify notation and denote this by $\sigma^2_t$

\item The mean and variance functions $\mu_t$ and $\sigma^2_t$ are handy 
  objects, because they tell us about salient features of the time series---the
  drift and spread, respectively, that we should expect over time

\item However, in general, they are not enough to characterize the entire
  distribution of the time series. Why? Two reasons: 

  \begin{enumerate}
  \item In general, the mean and variance are not enough to characterize the
    \emph{marginal} distribution of a single variate $x_t$ along the sequence 

  \item Furthermore, they say nothing about the \emph{joint} distribution of
    two variates $x_s$ and $x_t$ at different times, $s \not= t$. (For example,
    do they tend to go up and down together, or do they tend to repel, or ... ?) 
  \end{enumerate}

  The second of these (joint dependence) we will address soon when we talk about
  auto-covariance and stationarity. The first (mean and variance specifying the 
  distribution) we will revisit later when we talk about Gaussian processes  

\item Before moving on though, let's look at some examples. First, let's
  consider white noise, which recall, refers to a sequence $x_t$, $t =
  1,2,3,\dots$ of uncorrelated random variables, with zero mean, and constant
  variance. Precisely,   
  \begin{gather*}
  \Cov(x_s, x_t) = 0, \quad \text{for all $s \not= t$} \\
  \E(x_t) = 0, \; \Var(x_t) = \sigma^2, \quad \text{for all $t$} 
  \end{gather*}
  So by definition (this one is kind of vacuous), we have mean function $\mu_t =
  0$ and variance function $\sigma^2_t = \sigma^2$, which are constant functions
  (do not vary in time)

\item How about a moving average of white noise, with window length 3? This is 
  \[
  y_t = \frac{1}{3} \Big( x_{t-1} + x_t + x_{t+1} \Big)
  \]
  Its mean function is
  \begin{align*}
  \mu_t &= \E(y_t)  \\
  &= \frac{1}{3} \Big( \E(x_{t-1}) + \E(x_t) + \E(x_{t+1}) \Big) \\ 
  &= \frac{1}{3} ( 0 + 0 + 0 ) \\
  &= 0
  \end{align*}
  Its variance function is
  \begin{align*}
  \sigma^2_t &= \Var(y_t) \\
  &= \frac{1}{9} \Big( \Var(x_{t-1}) + \Var(x_t) + \Var(x_{t+1}) +{} \\
  &\qquad\qquad 2 \Cov(x_{t-1}, x_t) + 2 \Cov(x_t, x_{t+1}) + 2 \Cov(x_{t-1}, 
    x_{t+1}) \Big) \\
  &= \frac{1}{9} ( \sigma^2 + \sigma^2 + \sigma^2 + 0 + 0 + 0) \\
  &= \frac{1}{3} \sigma^2
  \end{align*}
  So its variance is smaller than that of original sequence. In short, smoothing
  reduces variance 

\item This last example might have helped you de-rust on some basic facts about
  expectations and variances. Recall, for constants $a_i$ and random variables
  $x_i$: 
  \begin{gather*}
  \E\bigg( \sum_{i=1}^n a_i x_i \bigg) = \sum_{i=1}^n a_i \E(x_i) \\
  \Var\bigg( \sum_{i=1}^n a_i x_i \bigg) = \sum_{i=1}^n a_i^2 \Var(x_i) + 2
  \sum_{i < j} \Cov(x_i, x_j) 
  \end{gather*}

\item The last rule can be thought of as a special case of the more general
  rule, for constants $a_i,b_j$, and random variables $x_i,y_j$: 
  \[
  \Cov\bigg( \sum_{i=1}^n a_i x_i , \sum_{j=1}^m b_j y_j  \bigg) 
  = \sum_{i,j} a_i b_j \Cov(x_i, y_j)
  \]
  (To be clear, the sum on the right-hand side above is taken over $i =
  1,\dots,n$ and $j = 1,\dots,m$) 

\item Ok, one last example before moving on: let's consider a random walk with
  drift,
  \[
  x_t = \delta + x_{t-1} + \epsilon_t
  \]
  for a white noise sequence $\epsilon_t$, $t = 1,2,3\dots$.  Recall, we can
  equivalently write this as (assuming we start at $x_0 = 0$):
  \[
  x_t = \delta t + \sum_{i=1}^t \epsilon_i 
  \] 
  From this, we can see that the mean function is 
  \[
  \mu_t = \delta t + \sum_{i=1}^t \E(\epsilon_i) = \delta t
  \]
  and the variance function is 
  \[
  \sigma^2_t = \sum_{i=1}^t \Var(\epsilon_i) + 2 \sum_{i<j} \Cov(\epsilon_i,
  \epsilon_j) = \sigma^2 t
  \]
  So both the mean and the variance grow over time, proportionally to $t$. 
  Figure \ref{fig:rw} plots example paths over multiple repetitions, for you to
  get a sense of this
\end{itemize}

\begin{figure}[p]
\centering
\includegraphics[width=0.875\textwidth]{fig/rw-1.pdf}
\includegraphics[width=0.875\textwidth]{fig/rw-2.pdf}
\caption{\it Random walk without and with drift, each with 100 sample paths. The
  darker, thicker line in each plot is the sample mean taken at each time point,
  with respect to the 100 repetitions.}
\label{fig:rw}
\end{figure}

\section{Covariance and correlation}

\subsection{Auto: one series}

\begin{itemize}
\item The \emph{auto-covariance function} associated with a time series $x_t$,
  $t=1,2,3,\dots$ is defined as
  \[
  \gamma_x(s,t) = \Cov(x_s, x_t)
  \]
  This is a symmetric function $\gamma_x(s,t) = \gamma_x(t,s)$, for all $s,t$.
  Note that of course $\gamma_x(t,t) = \sigma^2_{x,t}$, the variance
  function. As before, we will drop the subscript when it is clear from the
  context what the underlying sequence is, and simply write $\gamma(s,t)$  

\item The \emph{auto-correlation function} is defined by diving the
  auto-covariance function by the product of the relevant standard deviations, 
  \[
  \rho_x(s,t) = \frac{\gamma_x(s,t)}{\sigma_{x,s} \, \sigma_{x,t}}
  \]
  which we abbreviate as $\rho(s,t)$ when the underlying sequence is clear from
  the context 

\item By the Cauchy-Schwarz inequality, which states that
  \[
  \Cov(x, y) \leq \sqrt{\Var(x) \Var(y)}
  \]
  for any random variables $x,y$, note that we always have
  \[
  \rho_x(s,t) \in [-1, 1]
  \]
  Typically the auto-correlation will lie strictly in between these limits. 
  (What would a sequence with auto-correlation identically equal to 1 look like?  
  Identically equal to $-1$?) 

\item Broadly speaking, the auto-covariance function measures the \emph{linear}
  dependence between variates along the series. If a series is very smooth, then
  the auto-covariance function will typically be large (and positive when $s,t$
  are close together, but it may be negative when $s,t$ are farther apart). If a 
  series is choppy, then the auto-covariance function will typically be close to
  zero 

\item Recall that uncorrelatedness is not the same as independence! So we can
  have $\gamma_x(s,t) = 0$ for all $s,t$, even if $x_t$, $t=1,2,3,\dots$ are not
  independent random variables. However, for a Gaussian sequence,
  uncorrelatedness implies independence

\item Let's return to our examples. For white noise, the auto-covariance
  function is identically zero, $\gamma(s,t) = 0$ for all $s,t$. Hence the same
  is true of the auto-correlation function

\item For a moving average of white noise, the auto-variance function decreases
  as the gap between $s$ and $t$ grows. For example, for  
  \[
  y_t = \frac{1}{3} \Big( x_{t-1} + x_t + x_{t+1} \Big)
  \]
  we have
  \begin{align*}
  \gamma(s,t) &= \Cov(y_s, y_t) \\
  &= \Cov \bigg( \frac{1}{3} \Big( x_{s-1} + x_s + x_{s+1} \Big), 
    \frac{1}{3} \Big( x_{t-1} + x_t + x_{t+1} \Big) \bigg) \\
  &= \begin{cases}
  \sigma^2 / 9 & s = t-2 \\
  2\sigma^2 / 9 & s = t-1 \\
  \sigma^2 / 3 & s = t \\
  2\sigma^2 / 9 & s = t+1 \\
  \sigma^2 / 9 & s = t+2 \\
  0 & \text{otherwise}
  \end{cases}
  \end{align*}
  (You can go through each case carefully, and use the formula for the
  covariance of linear combinations given previously.) The auto-correlation
  function simply divides this by $\sigma^2$. since the variance function is 
  constant, and is hence
  \[
  \rho(s,t) = 
  \begin{cases}
  1 / 9 & s = t-2 \\
  2 / 9 & s = t-1 \\
  1 / 3 & s = t \\
  2 / 9 & s = t+1 \\
  1 / 9 & s = t+2 \\
  0 & \text{otherwise}
  \end{cases}
  \]

\item For a random walk (with or without drift), the auto-covariance function
  also decreases as the gap between $s$ and $t$ grows, but has a different 
  structure. Considering 
  \[
  x_t = \delta t + \sum_{i=1}^t \epsilon_i
  \]
  we have
  \begin{align*}
  \gamma(s,t) &= \Cov(x_s, x_t) \\
  &= \Cov \bigg( \delta s + \sum_{i=1}^s \epsilon_i, 
  \delta t + \sum_{i=1}^t \epsilon_i \bigg) \\
  &= \sigma^2 \min\{s, t\}
  \end{align*}
  (To see this more clearly, consider the case where $s<t$ and recognize that 
  the sums above overlap with exactly $s$ white noise variates.) The
  auto-correlation function divides this by the product of the relevant
  variances: 
  \begin{align*}
  \rho(s,t) &= \frac{\sigma^2 \min\{s, t\}}{\sigma \sqrt{s} \cdot \sigma 
              \sqrt{t}} \\  
  &= \frac{\min\{s, t\}}{\sqrt{st}}
  \end{align*}

\item Figure \ref{fig:autocor} gives a visualization of the auto-correlation
  functions for the moving average and random walk settings. The moving average
  auto-correlation function is presented as a banded matrix (though it is hard
  to see the band since the sequence is of total length $n=500$ and most values
  in the auto-correlation matrix are zero). Importantly, we can see that the
  same pattern persists throughout the whole matrix, and all that matters is the 
  distance to the diagonal. This is an important property that we will revisit
  soon (hint: stationarity). Meanwhile, the random walk auto-correlation
  function does \emph{not} have a pattern than persists throughout, and we can
  see a ``cone'' that grows around the diagonal as time grows
\end{itemize}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{fig/autocor-1.pdf}
\caption{\it Heatmaps of the auto-correlation functions for the moving average 
  and random walk examples, for a sequence with $n=500$ time points. The  
  heatmaps are laid out in the same way that we would naturally view a matrix:
  $(s,t) = (1,1)$ is the top left corner, and $(s,t) = (n,n)$ is the bottom
  right (that is, $s$ increases along the rows, and $t$ increases along the
  columns). Yellow reflects a value of zero, and darker red reflects a larger
  value.} 
\label{fig:autocor}
\end{figure}

\subsection{Cross: two series}

\begin{itemize}
\item The \emph{cross-covariance function} associated with two time series $x_t$, 
  $t=1,2,3,\dots$ and $y_t$, $t=1,2,3,\dots$ is defined as
  \[
  \gamma_{xy}(s,t) = \Cov(x_s, y_t)
  \]
  This is \emph{not} necessarily a symmetric function, and generically
  $\gamma_{xy}(s,t) \not= \gamma_{xy}(t,s)$. Note that the cross-covariance
  between a time series and itself is simply its auto-covariance, i.e.,
  $\gamma_{xx}(t,t) = \gamma_x(t)$

\item The \emph{cross-correlation function} is defined by diving the
  cross-covariance function by the product of the relevant standard deviations, 
  \[
  \rho_{xy}(s,t) = \frac{\gamma_{xy}(s,t)}{\sigma_{x,s} \, \sigma_{y,t}}
  \]
  By Cauchy-Schwarz, once again, we know that $\rho_{xy}(s,t) \in [-1,1]$

\item Figure \ref{fig:crosscor} shows an example of an \emph{estimated}
  cross-correlation function for Covid-19 cases (the first series $x_s$) and 
  deaths (the second series $y_t$) in California, which are plotted in Figure
  \ref{fig:covid}. We can see that the cross-correlation is plotted as a
  function of ``lag'', which refers to the value $h = s-t$, and appears to be  
  maximized at a lag of $h = -25$ or so. This makes sense, in that we expect
  cases to be highly correlated with deaths several weeks later (this is also
  visually apparent in Figure \ref{fig:covid})  

  But wait a minute ... why have we reduced the whole cross-correlation
  function, which is generically a function of two time indexes $s$ and $t$, to
  be a function of a single number, lag, $h = t-s$? Because that is really the
  only way it is estimable (unless we have more information than the two time
  series at hand). More on this shortly, but next, we'll cover stationarity,
  which will provide the foundation for this estimation strategy in the first
  place 
\end{itemize}

\begin{figure}[p]
\centering
\includegraphics[width=0.85\textwidth]{fig/covid-1.pdf}
\caption{\it  Covid-19 cases and deaths, in the state of California.}
\label{fig:covid}

\medskip
\includegraphics[width=0.875\textwidth]{fig/covid-2.pdf}
\caption{\it Cross-correlation function for Covid-19 cases and deaths in
  California, as plotted above. This is estimated by the \texttt{ccf()} function
  in R.}   
\label{fig:crosscor}
\end{figure}

\section{Stationarity}

\subsection{Strong}

\subsection{Weak}

\section{Estimation}

\section{Gaussian processes}

\end{document}